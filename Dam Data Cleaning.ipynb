{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework we'll be looking at the data released by the State Bank of Pakistan. The data is the daily bank-wise and donor-wise receipts of the fund for the Daimer Bhasha and Mohmand Dam. You can find them in the following link: http://www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm. Take a moment to look around the data and try to figure out what the possible challenges could be.\n",
    "\n",
    "The main purpose of this homework is to teach how to scrape data from the web, clean it, and import it into Pandas for data analysis purposes. There are however some things to note:\n",
    "1. As you can tell, the data is in PDF form. PDF is the most difficult to handle data format and if you get extremely broken CSV files, there isn't a need to worry, that's where the cleaning part comes in.\n",
    "2. We'll be using an API to convert the data from PDF to CSV, and then from CSV to Pandas. There are, however, other ways to do this. The reason we wanted to do this method is two-fold\n",
    "    * It will teach you how to communicate with APIs using Python, which will be a useful skill when you want to deploy your data models as an API so that it can work with other APIs that need those data models. Moreover, a lot of data you get in the real world is from APIs. \n",
    "    * The CSV will be extremely inconsistent, so it will give you immense practice with using regular-expressions, which are extremely important in the Data Science tool-kit.\n",
    "    \n",
    "Submit the notebooks in a similar format to the Labs: print the relevant output in each cell **only if it has an output. The initial scraping and converting does not have any output**, and name the notebooks as:\n",
    "**rollnumber_HW1.ipynb** for e.g **20100237_H1.ipynb**\n",
    "\n",
    "Please make sure you complete full parts (denoted by a Header each in this notebook) as the grading will be based on parts. Needless to say, do not copy someone else's code. In most Data Science careers, the main skill is not how good you are at coding, but how well you are able to use the tools at your disposal and what inferences you are able to make with the information that you have. Thus, while you might be able to do the HW by looking at someone else's code, unless you go through the actual thought process, you won't learn a lot.\n",
    "\n",
    "We'll be using a lot of libraries in this tutorial, make sure you go through them so you understand what they are used for.\n",
    "\n",
    "**NOTE: If you are more comfortable doing so, as I am, you can do the assignment on your preferred text editor on simple Python and then write the code neatly in a notebook.** Personally, I find Sublime/Vim easier to use than Jupyter, mostly since a lot of shortcuts there make coding much easier, while here the shortcuts are more about navigation and controlling your cells.\n",
    "\n",
    "**The homework is to be done in pairs of 2.** \n",
    "\n",
    "**Naming convention: rollnumber1_rollnumber2_HW1.ipynb**\n",
    "\n",
    "Total Marks: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Getting the Data\n",
    "\n",
    "You can have a look at the data through the link given above. Download a few PDF files and go through the data to see what it looks like. How many columns are there, each, in the PDF files? Are there any inconsistencies? Any particular values that pop out that would need to be taken care of later in your cleaning? Think of all these questions when going through the initial PDF because they will prove really helpful when you can not figure out why there are so many \"NaN\" values in your final DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Scraping              \n",
    "Marks: 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using what is called the *requests* model to get an HTML page, and then use *BeautifulSoup* to parse that HTML page such that we are able to to derive the appropriate information from it. I recommend you go through the documentation of each to learn more about how to use the libraries. \n",
    "\n",
    "* [Requests Documentation](http://docs.python-requests.org/en/master/)\n",
    "* [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [BeautifulSoup + Requests Tutorial](https://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup)\n",
    "* [BeautifulSoup](https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe) Note that this tutorial is more detailed. I would highly recommend you go through this as well even though the library used here is urllib2 instead of requests (which you can do as well!). It also links to more web-scraping libraries like Scrapy for more complicated scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# os is being imported so you can make a new directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saad Khan\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Saad Khan\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "## Write code here that will:\n",
    "    # Open each PDF link\n",
    "    # Save the PDF in a directory in the same folder \n",
    "    \n",
    "    \n",
    "# Your code goes here #\n",
    "r = requests.get(\"http://www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm\")\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data)\n",
    "pdf_list = []\n",
    "for link in soup.find_all('a'):\n",
    "    s = link.get('href')\n",
    "    t = s[-3:]\n",
    "    if t==\"pdf\":\n",
    "        s = \"http://www.sbp.org.pk/notifications/FD/DamFund/\" + s\n",
    "        pdf_list.append(s)\n",
    "for url in pdf_list:\n",
    "    r = requests.get(url)\n",
    "    open(url[-14:], 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Converting from PDF to CSV\n",
    "Marks: 15\n",
    "\n",
    "You have two possible options between deciding what API to use for the conversion task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first option is communicating with an API called [Zamzar](https://www.zamzar.com) to send each PDF, ask them to convert it into CSV, and then download the converted CSV. They provide sample code to do everything from generating a simple request to starting a conversion job, checking for completion, and then downloading the finished file. You can find this information on the [Zamzar Documentation](https://developers.zamzar.com/docs) page.\n",
    "\n",
    "**Important Information: **\n",
    "\n",
    "The API only provides 100 points of free conversion, and each PDF to CSV conversion costs 3 points, that means with one account you can only convert **33** PDFs. However, this also means you have very little room to play around with this API, unless you have an extra email-address, so you need to be very careful when coding to communicate with this API. \n",
    "\n",
    "Moreover, the API only keeps the converted files for one day with a free account, so make sure you do this part in one go.\n",
    "\n",
    "**Note: Using the Zamzar API grants a bonus of 10 marks. This will help if you are not able to complete this assignment, or it can be used up in a later assignment if you get 110/100 marks in this one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another extremely simple API is the [PDF Tables](https://pdftables.com) API which is much simpler to use than the Zamzar API, however does not allow you to check the job for completion or for any intermediate steps. Moreover, this requires the installation of a library. Once again, they allow only 50 versions for free, but that is enough conversions for us. This [blog post](https://pdftables.com/blog/pdf-to-excel-with-python) will help you figure out how to convert the PDF to CSV using Python.\n",
    "\n",
    "The cons of this API is that it will not really teach you any proper API communcation through requests since you do not have to navigate through any requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.auth import HTTPBasicAuth\n",
    "#import config\n",
    "import glob\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-08-2018.pdf\n",
      "3972553\n",
      "01-10-2018.pdf\n",
      "3972554\n",
      "02-08-2018.pdf\n",
      "3972555\n",
      "02-10-2018.pdf\n",
      "3972557\n",
      "03-08-2018.pdf\n",
      "3972559\n",
      "03-09-2018.pdf\n",
      "3972560\n",
      "03-10-2018.pdf\n",
      "3972561\n",
      "04-09-2018.pdf\n",
      "3972562\n",
      "04-10-2018.pdf\n",
      "3972563\n",
      "05-09-2018.pdf\n",
      "3972564\n",
      "06-07-2018.pdf\n",
      "3972566\n",
      "06-08-2018.pdf\n",
      "3972567\n",
      "06-09-2018.pdf\n",
      "3972568\n",
      "07-08-2018.pdf\n",
      "3972569\n",
      "07-09-2018.pdf\n",
      "3972570\n",
      "08-08-2018.pdf\n",
      "3972572\n",
      "09-07-2018.pdf\n",
      "3972573\n",
      "09-08-2018.pdf\n",
      "3972574\n",
      "10-07-2018.pdf\n",
      "3972575\n",
      "10-08-2018.pdf\n",
      "3972576\n",
      "10-09-2018.pdf\n",
      "3972577\n",
      "11-07-2018.pdf\n",
      "3972578\n",
      "11-09-2018.pdf\n",
      "3972579\n",
      "12-07-2018.pdf\n",
      "3972580\n",
      "12-09-2018.pdf\n",
      "3972581\n",
      "13-07-2018.pdf\n",
      "3972582\n",
      "13-08-2018.pdf\n",
      "3972583\n",
      "13-09-2018.pdf\n",
      "3972584\n",
      "14-09-2018.pdf\n",
      "3972586\n",
      "15-08-2018.pdf\n",
      "3972587\n",
      "16-07-2018.pdf\n",
      "3972588\n",
      "16-08-2018.pdf\n",
      "3972589\n",
      "17-07-2018.pdf\n",
      "3972590\n",
      "17-08-2018.pdf\n",
      "3972591\n",
      "17-09-2018.pdf\n",
      "3972592\n",
      "18-07-2018.pdf\n",
      "3972594\n",
      "18-09-2018.pdf\n",
      "3972595\n",
      "19-07-2018.pdf\n",
      "3972596\n",
      "19-09-2018.pdf\n",
      "3972598\n",
      "20-07-2018.pdf\n",
      "3972599\n",
      "20-08-2018.pdf\n",
      "3972600\n",
      "23-07-2018.pdf\n",
      "3972601\n",
      "24-07-2018.pdf\n",
      "3972603\n",
      "24-08-2018.pdf\n",
      "3972604\n",
      "24-09-2018.pdf\n",
      "3972605\n",
      "25-09-2018.pdf\n",
      "3972606\n",
      "26-07-2018.pdf\n",
      "3972607\n",
      "26-09-2018.pdf\n",
      "3972609\n",
      "27-07-2018.pdf\n",
      "3972610\n",
      "27-08-2018.pdf\n",
      "3972611\n",
      "27-09-2018.pdf\n",
      "3972612\n",
      "28-08-2018.pdf\n",
      "3972613\n",
      "28-09-2018.pdf\n",
      "3972614\n",
      "29-08-2018.pdf\n",
      "3972615\n",
      "30-07-2018.pdf\n",
      "3972616\n",
      "30-08-2018.pdf\n",
      "3972617\n",
      "31-07-2018.pdf\n",
      "3972618\n",
      "31-08-2018.pdf\n",
      "3972619\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "job_ids = []\n",
    "pdfs_folder = './*.pdf'\n",
    "api_key = 'f31e758ad5db7016a3fd65d1a75a069af063a774'\n",
    "api_key1 = '2c71d9e75c6e025365932a2d74c54c347f75dcf7'\n",
    "endpoint = \"https://api.zamzar.com/v1/jobs\"\n",
    "target_format = \"csv\"\n",
    "data_content = {'target_format': target_format}\n",
    "# You need a list to store all the job_ids from the response of posting the conversion job, \n",
    "# if you are using the Zamzar API\n",
    "# This piece of code shows you what glob does\n",
    "# Write code here to post a job, and append each job's id into job_ids ##\n",
    "mykey = api_key\n",
    "count = 0\n",
    "for file_name in glob.glob(pdfs_folder):\n",
    "    file_name = file_name[2:]\n",
    "    statinfo = os.stat(file_name)\n",
    "    if count>30:\n",
    "        mykey = api_key1\n",
    "#    if statinfo.st_size <=1000000:\n",
    "    file_content = {'source_file': open(file_name, 'rb')}\n",
    "    res = requests.post(endpoint, data=data_content, files=file_content, auth=HTTPBasicAuth(mykey, ''))\n",
    "    count+=1\n",
    "    job_ids.append(res.json()['id'])\n",
    "    print(file_name)\n",
    "    print(res.json()['id'])\n",
    "#    else:\n",
    "#        print(file_name)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this cell write the code to download the completed files. First check if a job_id's status is completed and wait until it is. After it has been completed, download the file and save it.\n",
    "\n",
    "The exact code required here is all in the documentation, the only additional task you have to do on your own is figure out a way to find out which file has just been received from the job_id, and name the local file.\n",
    "\n",
    "**Please look at the Example JSON response in the [documentation](https://developers.zamzar.com/docs) to learn how to figure out the filenames, job status etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3972589\n",
      "{'id': 3972589, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:10Z', 'finished_at': '2018-10-07T04:58:16Z', 'source_file': {'id': 39808567, 'name': '16-08-2018.pdf', 'size': 378150}, 'target_files': [{'id': 39808573, 'name': '16-08-2018.csv', 'size': 97956}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "16-08-2018.csv\n",
      "File downloaded\n",
      "3972594\n",
      "{'id': 3972594, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:23Z', 'finished_at': '2018-10-07T04:58:31Z', 'source_file': {'id': 39808577, 'name': '18-07-2018.pdf', 'size': 296432}, 'target_files': [{'id': 39808582, 'name': '18-07-2018.csv', 'size': 179053}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "18-07-2018.csv\n",
      "File downloaded\n",
      "3972598\n",
      "{'id': 3972598, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:33Z', 'finished_at': '2018-10-07T04:58:45Z', 'source_file': {'id': 39808584, 'name': '19-09-2018.pdf', 'size': 630169}, 'target_files': [{'id': 39808593, 'name': '19-09-2018.csv', 'size': 172977}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "19-09-2018.csv\n",
      "File downloaded\n",
      "3972600\n",
      "{'id': 3972600, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:38Z', 'finished_at': '2018-10-07T04:58:48Z', 'source_file': {'id': 39808588, 'name': '20-08-2018.pdf', 'size': 527685}, 'target_files': [{'id': 39808596, 'name': '20-08-2018.csv', 'size': 133374}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "20-08-2018.csv\n",
      "File downloaded\n",
      "3972603\n",
      "{'id': 3972603, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:44Z', 'finished_at': '2018-10-07T04:58:50Z', 'source_file': {'id': 39808594, 'name': '24-07-2018.pdf', 'size': 495082}, 'target_files': [{'id': 39808598, 'name': '24-07-2018.csv', 'size': 113133}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "24-07-2018.csv\n",
      "File downloaded\n",
      "3972605\n",
      "{'id': 3972605, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:51Z', 'finished_at': '2018-10-07T04:59:01Z', 'source_file': {'id': 39808599, 'name': '24-09-2018.pdf', 'size': 603587}, 'target_files': [{'id': 39808605, 'name': '24-09-2018.csv', 'size': 187939}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "24-09-2018.csv\n",
      "File downloaded\n",
      "3972607\n",
      "{'id': 3972607, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:57Z', 'finished_at': '2018-10-07T04:59:01Z', 'source_file': {'id': 39808602, 'name': '26-07-2018.pdf', 'size': 425415}, 'target_files': [{'id': 39808607, 'name': '26-07-2018.csv', 'size': 93026}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "26-07-2018.csv\n",
      "File downloaded\n",
      "3972610\n",
      "{'id': 3972610, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:02Z', 'finished_at': '2018-10-07T04:59:06Z', 'source_file': {'id': 39808606, 'name': '27-07-2018.pdf', 'size': 171967}, 'target_files': [{'id': 39808611, 'name': '27-07-2018.csv', 'size': 84430}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "27-07-2018.csv\n",
      "File downloaded\n",
      "3972612\n",
      "{'id': 3972612, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:08Z', 'finished_at': '2018-10-07T04:59:14Z', 'source_file': {'id': 39808612, 'name': '27-09-2018.pdf', 'size': 465025}, 'target_files': [{'id': 39808616, 'name': '27-09-2018.csv', 'size': 111709}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "27-09-2018.csv\n",
      "File downloaded\n",
      "3972614\n",
      "{'id': 3972614, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:14Z', 'finished_at': '2018-10-07T04:59:23Z', 'source_file': {'id': 39808615, 'name': '28-09-2018.pdf', 'size': 421083}, 'target_files': [{'id': 39808620, 'name': '28-09-2018.csv', 'size': 81625}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "28-09-2018.csv\n",
      "File downloaded\n",
      "3972616\n",
      "{'id': 3972616, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:21Z', 'finished_at': '2018-10-07T04:59:27Z', 'source_file': {'id': 39808619, 'name': '30-07-2018.pdf', 'size': 596746}, 'target_files': [{'id': 39808624, 'name': '30-07-2018.csv', 'size': 146976}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "30-07-2018.csv\n",
      "File downloaded\n",
      "3972618\n",
      "{'id': 3972618, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:28Z', 'finished_at': '2018-10-07T04:59:33Z', 'source_file': {'id': 39808623, 'name': '31-07-2018.pdf', 'size': 491948}, 'target_files': [{'id': 39808627, 'name': '31-07-2018.csv', 'size': 127422}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "31-07-2018.csv\n",
      "File downloaded\n",
      "3972591\n",
      "{'id': 3972591, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:16Z', 'finished_at': '2018-10-07T04:58:23Z', 'source_file': {'id': 39808572, 'name': '17-08-2018.pdf', 'size': 437923}, 'target_files': [{'id': 39808578, 'name': '17-08-2018.csv', 'size': 95895}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "17-08-2018.csv\n",
      "File downloaded\n",
      "3972599\n",
      "{'id': 3972599, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:35Z', 'finished_at': '2018-10-07T04:58:41Z', 'source_file': {'id': 39808586, 'name': '20-07-2018.pdf', 'size': 123637}, 'target_files': [{'id': 39808591, 'name': '20-07-2018.csv', 'size': 110037}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "20-07-2018.csv\n",
      "File downloaded\n",
      "3972604\n",
      "{'id': 3972604, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:46Z', 'finished_at': '2018-10-07T04:58:53Z', 'source_file': {'id': 39808595, 'name': '24-08-2018.pdf', 'size': 459965}, 'target_files': [{'id': 39808601, 'name': '24-08-2018.csv', 'size': 115612}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "24-08-2018.csv\n",
      "File downloaded\n",
      "3972609\n",
      "{'id': 3972609, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:00Z', 'finished_at': '2018-10-07T04:59:05Z', 'source_file': {'id': 39808604, 'name': '26-09-2018.pdf', 'size': 428341}, 'target_files': [{'id': 39808610, 'name': '26-09-2018.csv', 'size': 123161}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "26-09-2018.csv\n",
      "File downloaded\n",
      "3972613\n",
      "{'id': 3972613, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:11Z', 'finished_at': '2018-10-07T04:59:22Z', 'source_file': {'id': 39808613, 'name': '28-08-2018.pdf', 'size': 601019}, 'target_files': [{'id': 39808618, 'name': '28-08-2018.csv', 'size': 136285}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "28-08-2018.csv\n",
      "File downloaded\n",
      "3972617\n",
      "{'id': 3972617, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:24Z', 'finished_at': '2018-10-07T04:59:30Z', 'source_file': {'id': 39808621, 'name': '30-08-2018.pdf', 'size': 495284}, 'target_files': [{'id': 39808626, 'name': '30-08-2018.csv', 'size': 93960}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "30-08-2018.csv\n",
      "File downloaded\n",
      "3972596\n",
      "{'id': 3972596, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:29Z', 'finished_at': '2018-10-07T04:58:36Z', 'source_file': {'id': 39808580, 'name': '19-07-2018.pdf', 'size': 309990}, 'target_files': [{'id': 39808587, 'name': '19-07-2018.csv', 'size': 176971}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "19-07-2018.csv\n",
      "File downloaded\n",
      "3972606\n",
      "{'id': 3972606, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:54Z', 'finished_at': '2018-10-07T04:59:03Z', 'source_file': {'id': 39808600, 'name': '25-09-2018.pdf', 'size': 599624}, 'target_files': [{'id': 39808608, 'name': '25-09-2018.csv', 'size': 172824}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "25-09-2018.csv\n",
      "File downloaded\n",
      "3972615\n",
      "{'id': 3972615, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:18Z', 'finished_at': '2018-10-07T04:59:25Z', 'source_file': {'id': 39808617, 'name': '29-08-2018.pdf', 'size': 549588}, 'target_files': [{'id': 39808622, 'name': '29-08-2018.csv', 'size': 119021}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "29-08-2018.csv\n",
      "File downloaded\n",
      "3972601\n",
      "{'id': 3972601, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:58:41Z', 'finished_at': '2018-10-07T04:58:49Z', 'source_file': {'id': 39808590, 'name': '23-07-2018.pdf', 'size': 275118}, 'target_files': [{'id': 39808597, 'name': '23-07-2018.csv', 'size': 146741}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "23-07-2018.csv\n",
      "File downloaded\n",
      "3972619\n",
      "{'id': 3972619, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:31Z', 'finished_at': '2018-10-07T04:59:37Z', 'source_file': {'id': 39808625, 'name': '31-08-2018.pdf', 'size': 395974}, 'target_files': [{'id': 39808628, 'name': '31-08-2018.csv', 'size': 83116}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "31-08-2018.csv\n",
      "File downloaded\n",
      "3972611\n",
      "{'id': 3972611, 'key': '2c71d9e75c6e025365932a2d74c54c347f75dcf7', 'status': 'successful', 'sandbox': False, 'created_at': '2018-10-07T04:59:05Z', 'finished_at': '2018-10-07T04:59:14Z', 'source_file': {'id': 39808609, 'name': '27-08-2018.pdf', 'size': 306180}, 'target_files': [{'id': 39808614, 'name': '27-08-2018.csv', 'size': 132495}], 'target_format': 'csv', 'credit_cost': 3}\n",
      "27-08-2018.csv\n",
      "File downloaded\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "## Your code goes here ##\n",
    "num = 0\n",
    "while job_ids:\n",
    "    for ids in job_ids:\n",
    "        endpoint = \"https://api.zamzar.com/v1/jobs/{}\".format(ids)\n",
    "        print(ids)\n",
    "        response = requests.get(endpoint, auth=HTTPBasicAuth(api_key1, ''))\n",
    "        print(response.json())\n",
    "        local_filename = response.json()['source_file']['name'][:-3]+\"csv\"\n",
    "        if response.json()['status']==\"successful\":\n",
    "            file_id = response.json()['target_files'][0]['id']\n",
    "            endpoint = \"https://api.zamzar.com/v1/files/{}/content\".format(file_id)\n",
    "            resp = requests.get(endpoint, stream=True, auth=HTTPBasicAuth(api_key1, ''))\n",
    "            job_ids.remove(ids)\n",
    "            try:\n",
    "                print(local_filename)\n",
    "                with open(local_filename, 'wb') as f:\n",
    "                    for chunk in resp.iter_content(chunk_size=1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            f.flush()\n",
    "                print(\"File downloaded\")\n",
    "                num+=1\n",
    "\n",
    "            except IOError:\n",
    "                print(\"Error\")\n",
    "\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parsing the CSV File\n",
    "Marks: 35\n",
    "\n",
    "This is perhaps the most difficult part of the assignment, you have to follow a similar strategy to what you did the Udacity Lab 1. You can not simply use Pandas read_csv since the conversion is not perfect and there will be rows with different number of columns, which Pandas does not take care of.\n",
    "\n",
    "### **Main Task:**\n",
    "* Write a function that parses a CSV into a Pandas DataFrame\n",
    "* Each DataFrame should consist of three columns with headers Bank, Donor_Name, and Amount\n",
    "* The date should be retrieved from the given filename \n",
    "* The Donor_Name can be NaN, as it is in a lot of cases. But try to retrieve as much information as possible\n",
    "* Remove all \"Page of\" rows\n",
    "* Don't include the header rows (e.g. \"SUPREME COURT FUND....\") into the DataFrame\n",
    "* The Amount should be converted into a Pandas numeric at the end\n",
    "\n",
    "### Other info:\n",
    "Some important resources for this part are (you can choose any one tutorial that you feel is easy to understand, they all cover roughly the same content):\n",
    "* [RegEx Tutorial 1](https://www.regular-expressions.info/)\n",
    "* [RegEx Tutorial 2](https://regexone.com/lesson/introduction_abcs)\n",
    "* [RegEx Tutorial 3](https://www.rexegg.com/)\n",
    "* [RegEx Cheatsheat](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285)\n",
    "* This [RegEx Editor](https://regex101.com/) is your best friend since you can test your expression separately on this\n",
    "\n",
    "You will probably have to use the CSV reader in order to get all the rows of the file. You can learn more about it using this [tutorial](https://www.alexkras.com/how-to-read-csv-file-in-python/).\n",
    "\n",
    "Some tips:\n",
    "* First find out how many columns are in each row\n",
    "* Print out rows which are longer than they should be (they should all be of length 3)\n",
    "* Try to find patterns in how the data is spread, and what common problems exist in all rows\n",
    "* Write some regex to try an extract the amount from the problem row and then:\n",
    "    * Put the amount as the third column\n",
    "    * Merge the rest of the string as a name of the donor in the 2nd column\n",
    "* Also check if the rows with 3 columns are correctly formatted or not, many of them would probably not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re # To use regular expressions\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#example_file = '17-08-2018.csv' # Assuming the file is in the folder all_csvs and is named appropriately\n",
    "# This is one of the most problematic files which is why I have included this in the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(filename):\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = [row for row in reader]\n",
    "        lst = []\n",
    "       \n",
    "        i = len(data)\n",
    "        check = 0\n",
    "        for x in range(i):\n",
    "            lst.append(x)\n",
    "            if data[x]:\n",
    "                if data[x][0]=='Bank' or data[x][0]=='Debit account description' or data[x][0]=='BANK':\n",
    "                    if data[x][0]=='Debit account description':\n",
    "                        check = 1\n",
    "                    break;\n",
    "        i = len(lst)\n",
    "        for x in lst:\n",
    "            data.remove(data[0])\n",
    "        if check==1:\n",
    "            for r in data:\n",
    "                t = r[1]\n",
    "                r.remove(r[1])\n",
    "                r.append(t)\n",
    "\n",
    "        for record in data:\n",
    "            if record[0][0:4]=='Page':\n",
    "                data.remove(record)\n",
    "            elif record[1][0:4]=='Page':\n",
    "                data.remove(record)\n",
    "            if record[0][-5:]=='Total':\n",
    "                data.remove(record)\n",
    "        for r in data:\n",
    "            item = r[0]\n",
    "            split1 = item.split('Limited')\n",
    "            split2 = item.split('Ltd')\n",
    "            split3 = item.split('(PAKISTAN)')\n",
    "            split5 = item.split('branches')\n",
    "            split6 = item.split('Branches')\n",
    "            split4 = item.split('Branch')\n",
    "            split7 = item.split('Pakistan')\n",
    "            if len(split1[0])!=len(item):\n",
    "                part1=split1[0] + 'Limited'\n",
    "                part2 = split1[1]\n",
    "                if part2:\n",
    "                    r.remove(r[0])\n",
    "                    r.insert(0,part2)\n",
    "                    r.insert(0,part1)\n",
    "            elif len(split2[0])!=len(item):\n",
    "                part1 = split2[0] + 'Ltd'\n",
    "                part2 = split2[1]\n",
    "                if part2:\n",
    "                    r.remove(r[0])\n",
    "                    r.insert(0,part2)\n",
    "                    r.insert(0,part1)\n",
    "            elif len(split6[0])!=len(item):\n",
    "                part1 = split6[0] + 'Branches'\n",
    "                part2 = split6[1]\n",
    "                if part2:\n",
    "                    r.remove(r[0])\n",
    "                    r.insert(0,part2)\n",
    "                    r.insert(0,part1)\n",
    "            elif len(split4[0])!=len(item):\n",
    "                part1 = split4[0] + 'Branch'\n",
    "                part2 = split4[1]\n",
    "                if part2:\n",
    "                    r.remove(r[0])\n",
    "                    r.insert(0,part2)\n",
    "                    r.insert(0,part1)\n",
    "            elif len(split5[0])!=len(item):\n",
    "                part1 = split5[0] + 'branches'\n",
    "                part2 = split5[1]\n",
    "                if part2:\n",
    "                    r.remove(r[0])\n",
    "                    r.insert(0,part2)\n",
    "                    r.insert(0,part1)\n",
    "            elif len(split7[0])!=len(item):\n",
    "                part1 = split7[0] + 'Pakistan'\n",
    "                part2 = split7[1]\n",
    "                if part2:\n",
    "                    r.remove(r[0])\n",
    "                    r.insert(0,part2)\n",
    "                    r.insert(0,part1)\n",
    "            \n",
    "            elif len(split3[0])!=len(item):\n",
    "                part1 = split3[0] + '(PAKISTAN)'\n",
    "                part2 = split3[1]\n",
    "                if part2:\n",
    "                    if len(part2)>4:\n",
    "                        r.remove(r[0])\n",
    "                        r.insert(0,part2)\n",
    "                        r.insert(0,part1)\n",
    "            else:\n",
    "                continue\n",
    "        for record in data:\n",
    "            if len(record)>3:\n",
    "                record[:] = [item for item in record if item != '']\n",
    "        \n",
    "        for r in data:\n",
    "            if len(r)>3:\n",
    "                result = ''.join([i for i in r[-1] if i.isdigit()])\n",
    "                if result!='':\n",
    "                        r[-1] = result\n",
    "                else:\n",
    "                    r.remove(r[-1])\n",
    "                    result = ''.join([i for i in r[-1] if i.isdigit()])\n",
    "                    r[-1] = result\n",
    "            else:\n",
    "                result = ''.join([i for i in r[-1] if i.isdigit()])\n",
    "                if result!='':\n",
    "                        r[-1] = result\n",
    "        if data:\n",
    "            if data[-1][0]=='Grand Total':\n",
    "                data.remove(data[-1])\n",
    "        \n",
    "        for r in data:\n",
    "            if len(r)>=2:\n",
    "                if r[1]==' Total':\n",
    "                    data.remove(r)\n",
    "        for r in data:\n",
    "            if len(r)>3:\n",
    "                if r[1]==r[2]:\n",
    "                    r.remove(r[2])\n",
    "        for r in data:\n",
    "            if len(r)>3:\n",
    "                l = len(r)-2\n",
    "                i = 1\n",
    "                temp = ''\n",
    "                for i in range(1,l):\n",
    "                    temp = temp+' '+r[2]\n",
    "                    r.remove(r[2])\n",
    "                    r[1]= r[1] +' '+ temp\n",
    "        for r in data:\n",
    "            if not r:\n",
    "                data.remove(r)\n",
    "        for r in data:\n",
    "            if len(r)==2:\n",
    "                r.insert(1,None)\n",
    "        for r in data:\n",
    "            if r[0]=='' and r[-1]==None:\n",
    "                data.remove(r)\n",
    "        list_remove=[]\n",
    "        for x in range(len(data)):\n",
    "            if data[x][0]==None or len(data[x])==1:\n",
    "                list_remove.append(x)\n",
    "        c = 0\n",
    "        for x in list_remove:\n",
    "            data.remove(data[x-c])\n",
    "            c+=1\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, remove headers and convert all amounts to Numeric; if it can't be converted it needs to be NaN\n",
    "def read_csv(filename):\n",
    "    headers = ['Bank', 'Donor_Name','Amount']\n",
    "    \n",
    "    raw_data = parser(filename)\n",
    "    date = filename[:-4]\n",
    "    dates_col = []\n",
    "    for i in range(len(raw_data)):\n",
    "        dates_col.append(date)\n",
    "    df = pd.DataFrame(raw_data, columns = headers)\n",
    "    df['Date']=dates_col\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Importing Full Dataset\n",
    "Marks: 10 \n",
    "\n",
    "The only additional task in this part is to:\n",
    "* Run the parser on all the files\n",
    "* For each file **add a 'Date' column, which should be inferred from the filename**\n",
    "* Concatenate each DataFrame into one large DataFrame. *Hint: concat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Bank        Donor_Name    Amount        Date\n",
      "0  AL BARAKA BANK (PAKISTAN) LTD       FARHAN ARIF  500000.0  01-08-2018\n",
      "1  AL BARAKA BANK (PAKISTAN) LTD       DAM UL HUDA  200000.0  01-08-2018\n",
      "2  AL BARAKA BANK (PAKISTAN) LTD       FARIS AHMED  200000.0  01-08-2018\n",
      "3  AL BARAKA BANK (PAKISTAN) LTD    MUHAMMAD AJMAL  100000.0  01-08-2018\n",
      "4  AL BARAKA BANK (PAKISTAN) LTD  SHAHEENA SULTANA  100000.0  01-08-2018\n",
      "(171992, 4)\n",
      "                            Bank       Donor_Name  Amount        Date\n",
      "171987  Zarai Taraqiati Bank Ltd           M KHAN   500.0  31-08-2018\n",
      "171988  Zarai Taraqiati Bank Ltd  INAYAT ALI KHAN   500.0  31-08-2018\n",
      "171989  Zarai Taraqiati Bank Ltd         M HAYYAT   800.0  31-08-2018\n",
      "171990  Zarai Taraqiati Bank Ltd    SYED RABNAWAZ  1000.0  31-08-2018\n",
      "171991  Zarai Taraqiati Bank Ltd          M USMAN  1300.0  31-08-2018\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('./*.csv')\n",
    "list_frames = []\n",
    "count = 0\n",
    "for f in files:\n",
    "    dff = read_csv(f[2:])\n",
    "    list_frames.append(dff)\n",
    "full_data = pd.concat(list_frames).reset_index(drop=True)\n",
    "full_data['Amount']=pd.to_numeric(full_data['Amount'], errors='coerce')\n",
    "#full_data['Date']=pd.to_datetime(full_data['Date'], format=\"%d-%m-%Y\")\n",
    "print (full_data.head())\n",
    "print (full_data.shape)\n",
    "print (full_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Integrity Checks\n",
    "Marks: 20\n",
    "\n",
    "* How many NaN values are there in each column? Why are they there? \n",
    "* What are the maximum and minimum values, is there anything peculiar about the max values?\n",
    "* Are there any rows which are not NaN but should still be a different DataFrame altogether?\n",
    "* Should these problem rows be removed? Can they be useful in other ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value is: \n",
      "Bank          Habib Bank Limited\n",
      "Donor_Name                  None\n",
      "Amount                3.4201e+30\n",
      "Date                  26-09-2018\n",
      "Name: 145860, dtype: object\n",
      "This could be a collective sum from a company's employees or something similar as there is no donor name.\n",
      "Min value is: \n",
      "Bank                 Faysal Bank Limited\n",
      "Donor_Name    MOHAMMAD HASSAAN BIN ZAFAR\n",
      "Amount                                 0\n",
      "Date                          04-09-2018\n",
      "Name: 19205, dtype: object\n",
      "It is possible that the person was a potential donor. Or, the records were in multiple rows, so they were not merged.\n"
     ]
    }
   ],
   "source": [
    "#Reason printed\n",
    "print(\"Max value is: \")\n",
    "print(full_data.loc[full_data['Amount'].idxmax()])\n",
    "print(\"This could be a collective sum from a company's employees or something similar as there is no donor name.\")\n",
    "print(\"Min value is: \")\n",
    "print(full_data.loc[full_data['Amount'].idxmin()])\n",
    "print(\"It is possible that the person was a potential donor. Or, the records were in multiple rows, so they were not merged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank             0\n",
      "Donor_Name    2653\n",
      "Amount        4462\n",
      "Date             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(full_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN in Donor_Name: People choose to be annonymous when sending out their donations. Or name was not noted. \n",
    "NaN in Amount: They could be potential donors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
